<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Cgx by hohoCode</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Cgx</h1>
        <p>UltraFast GPU Grammar Extractor for Statistical Machine Translation</p>
        <p class="view"><a href="https://github.com/hohoCode/cgx">View the Project on GitHub <small>hohoCode/cgx</small></a></p>
      </header>
      <section>
        <h1>
<a id="cgx" class="anchor" href="#cgx" aria-hidden="true"><span class="octicon octicon-link"></span></a>cgx</h1>

<p>UltraFast GPU Grammar Extractor for Statistical Machine Translation</p>

<h2>
<a id="installation-instruction" class="anchor" href="#installation-instruction" aria-hidden="true"><span class="octicon octicon-link"></span></a>INSTALLATION Instruction</h2>

<ul>
<li><p>please download thrust GPU library, which actually just needs git clone and that is it. Address is here: <a href="https://github.com/thrust/thrust">https://github.com/thrust/thrust</a></p></li>
<li><p>Install CUDA library on your server (soemtimes this step people just stuck here I see many of issues in this step for example on stackoverflow).</p></li>
<li><p>Would require the GPU device to have at least 3GB GPU memory. and latest GPU (Kephler architecture)  the best. But these codes should work with Kephler/Fermi/pre-Fermi architecture GPUs.</p></li>
<li><p>In provided MAKEFILE file, we have:</p></li>
</ul>

<pre><code>NVCC =nvcc -arch=compute_35 -code=sm_35
CUDA_INSTALL_PATH= /opt/common/cuda/cuda-5.5.22
OPT = -O3 -I./uthash/ -I/scratch0/huah/thrust/
NVCCFLAGS = $(OPT) -use_fast_math -I. -I$(CUDA_INSTALL_PATH)/include 
</code></pre>

<ul>
<li><p>The above four variables in MAKEFILE need to be updated accoring to your runnning enviroment. For example, the CUDA library install path ($CUDA_INSTALL_PATH) needs to be set to the corresponding path on your GPU server; $OPT needs thrust library directory's path; The computing version of your GPU device should be set (For example, Tesla K20 is 3.5 therfore it is -arch=compute_35), etc. </p></li>
<li><p>In the main directory please compile the codes as below. Probably will see lots of warnings please just ignore those as long as there are no errors. If errors that could probably be CUDA library related issues, Please update your cuda driver and library to the latest version.</p></li>
</ul>

<pre><code>make
</code></pre>

<ul>
<li>If you can get this executable file under bin/ directory, that means compilation is good:</li>
</ul>

<pre><code>bin/strmatchcuda
</code></pre>

<ul>
<li>Command to run (one example on FBIS parallel data):</li>
</ul>

<pre><code>./bin/strmatchcuda 
fbis.zh 
allqueries.txt.2000 
fbis.en
fbis.aligned 
lex.bin 
gpugrammar_temp
</code></pre>

<ul>
<li>
<p>Please provide the following as input arguments:</p>

<ol>
<li>First argument is the address of chinese side of parallel corpus.</li>
<li>Second: query file (each line is a query)</li>
<li>Third: english side of FBIS data</li>
<li>Four: Alignment file</li>
<li>Five: lexical bin (which is directly from cdec precomputation step on FBIS data, just reuse its output for lexical features)</li>
<li>Six: the output directory address. (this is the directory address to hold the output grammar files. Each query will output one grammar file. So all grammars will be in the directory). If this directory does not exist yet please create this directory first.</li>
<li>Note the above example just assume these files are on the same directory, while actually they can be anywhere just specify the correct address. The above files are having the same file format as before.</li>
</ol>
</li>
<li><p>So in the end if you see output log is like the below, which means everything has been done and it is in the last printing step ('IO step' as in the paper):</p></li>
</ul>

<pre><code>Start Printing Gappy Phrases...
</code></pre>

<ul>
<li>Once done just go check the  gpugrammar_temp directory and there should be bunch of grammar files for queries (one file for each query).</li>
<li>Troubleshooting. Common installation problems are mainly from CUDA memory allocation/access side, causing weido memory problems during running. We encourage you to use the latest GPU driver and CUDA library.</li>
<li>We will further update our codes significantly in our next release to make this code easier to use.. Stay tuned.</li>
</ul>

<p>Thanks.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/hohoCode">hohoCode</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
